#!/bin/bash

set -ex

SERVER_IP=10.111.112.100

# static host name and IP so that peer VMs can find us
systemctl enable --now NetworkManager
nmcli con add con-name "static-mcast1" ifname ens15 type ethernet ip4 "$SERVER_IP/20" ipv4.dns "$SERVER_IP" gw4 "10.111.112.1"
nmcli con up "static-mcast1"
hostnamectl set-hostname services.cockpit.lan

#############
#
# FreeIPA setup
#
#############

# see https://quay.io/repository/freeipa/freeipa-server
# and https://github.com/freeipa/freeipa-container/issues/346
# ideally we'd use the :fedora-rawhide stream, but version 4.9.7 breaks compatibility with client-side packages
# (https://bugzilla.redhat.com/show_bug.cgi?id=2015102), so use more stable centos-8-stream for the time being
setsebool -P container_manage_cgroup 1
mkdir /var/lib/ipa-data

cat <<EOF > /root/run-freeipa
podman run -d --rm --name freeipa -ti -h f0.cockpit.lan \
    -e IPA_SERVER_IP=$SERVER_IP \
    -p $SERVER_IP:53:53/udp -p $SERVER_IP:53:53 -p 80:80 -p 443:443 -p 389:389 -p 636:636 -p 88:88 -p 464:464 -p 88:88/udp -p 464:464/udp -p 123:123/udp \
    -v /var/lib/ipa-data:/data:Z \
    -v /sys/fs/cgroup:/sys/fs/cgroup:ro \
    quay.io/freeipa/freeipa-server:centos-8-stream \
    -U -p foobarfoo -a foobarfoo -n cockpit.lan -r COCKPIT.LAN --setup-dns --no-forwarders --no-ntp
EOF
chmod 755 /root/run-freeipa
/root/run-freeipa

podman logs -f freeipa &
LOGS=$!

# so wait until booted and setup is done
podman exec freeipa sh -ec 'until systemctl --quiet is-system-running; do sleep 5; done'

# stop podman logs
kill $LOGS
wait || true

# further setup
podman exec freeipa sh -exc '
# Default password expiry of 90 days is impractical
echo foobarfoo | kinit admin@COCKPIT.LAN
ipa pwpolicy-mod --minlife=0 --maxlife=1000
# Change password to apply new password policy
printf "foobarfoo\nfoobarfoo\n" | ipa user-mod --password admin
# Allow "admins" IPA group members to run sudo
# This is an "unbreak my setup" step and ought to happen by default.
# See https://pagure.io/freeipa/issue/7538
ipa-advise enable-admins-sudo | sh -ex
ipa dnsconfig-mod --forwarder=8.8.8.8
poweroff
'

#############
#
# Samba AD setup
#
#############

cat <<EOF > /root/samba-ad.json
{
  "samba-container-config": "v0",
  "configs": {
    "demo": {
      "instance_features": ["addc"],
      "domain_settings": "sink",
      "instance_name": "f0"
    }
  },
  "domain_settings": {
    "sink": {
      "realm": "COCKPIT.LAN",
      "short_domain": "COCKPIT",
      "admin_password": "foobarFoo123"
    }
  }
}
EOF

# See https://github.com/samba-in-kubernetes/samba-container#ad-dc
podman pull quay.io/samba.org/samba-ad-server

cat <<EOF > /root/run-samba-domain
# conflicts with samba's DNS
systemctl stop systemd-resolved
podman run -d -it --rm --name samba \
    --privileged --network=host \
    -v /root/samba-ad.json:/etc/samba/container.json \
    -h f0.cockpit.lan \
    quay.io/samba.org/samba-ad-server
EOF
chmod 755 /root/run-samba-domain

# no need to run the script here; it initializes reasonably fast and we don't have post-setup to do for now


#############################
#
# candlepin setup
#
#############################

# run the candlepin container to copy the certificates out of it
podman run -d --name candlepin \
    --uts=host \
    -p 8443:8443 \
    ghcr.io/ptoscano/candlepin-unofficial:latest

# give systemd the time to start in the container
sleep 5

podman exec -i candlepin sh -eux <<EOF
# ensure hostname is installed, as it is used by gen_certs.sh
dnf --setopt install_weak_deps=False -y install hostname
# stop tomcat, so we can regenerate the certs
systemctl stop tomcat
# regenerate the certs used by Candlepin; this is done so the hostname
# of the certificate is actually this container's hostname (which actually
# is the hostname of the system, because of --uts=host)
~candlepin/devel/candlepin/bin/deployment/gen_certs.sh -f
# restart tomcat
systemctl start tomcat
EOF

# give candlepin the time to start in the container
sleep 5

# validate that it works
until curl --insecure --fail --head https://localhost:8443/candlepin/status; do sleep 5; done

# copy the certificate to where the tests expect them
mkdir -p /home/admin/candlepin
podman cp candlepin:/home/candlepin/devel/candlepin/generated_certs /home/admin/candlepin/
mkdir -p /home/admin/candlepin/certs
podman cp candlepin:/etc/candlepin/certs/candlepin-ca.crt /home/admin/candlepin/certs/
podman cp candlepin:/etc/candlepin/certs/candlepin-ca-pub.key /home/admin/candlepin/certs/
chown -R admin:admin /home/admin/candlepin/

podman stop candlepin

cat <<EOF > /root/run-candlepin
#!/bin/sh
podman start candlepin
EOF
chmod 755 /root/run-candlepin

#############################
#
# grafana setup
#
#############################

# This does not actually prevent the "Failed to fetch plugins from catalog" error, but at least we try..
cat <<EOF > /root/grafana.ini
[analytics]
enabled = false
reporting_enabled = false
check_for_updates = false
check_for_plugin_updates = false
EOF
chmod 644 /root/grafana.ini

# enable PCP plugin
cat <<EOF > /root/pcp.yaml
apiVersion: 1
apps:
  - type: performancecopilot-pcp-app
EOF

podman run -d --rm --name grafana -p 3000:3000 \
    -v /root/grafana.ini:/opt/bitnami/grafana/conf/grafana.ini:ro,z \
    -v grafana-data-plugins:/opt/bitnami/grafana/data/plugins \
    -e GF_SECURITY_ADMIN_PASSWORD=foobar \
    -e GF_INSTALL_PLUGINS="redis-datasource,performancecopilot-pcp-app" \
    docker.io/bitnami/grafana

# wait until set up completed
until curl http://localhost:3000; do sleep 5; done
podman stop grafana

cat <<EOF > /root/run-grafana
#!/bin/sh
podman run -d --rm --name grafana -p 3000:3000 \
    -v /root/grafana.ini:/opt/bitnami/grafana/conf/grafana.ini:z \
    -v /root/pcp.yaml:/opt/bitnami/grafana/conf/provisioning/plugins/pcp.yaml:ro,z \
    -v grafana-data-plugins:/opt/bitnami/grafana/data/plugins \
    -e GF_SECURITY_ADMIN_PASSWORD=foobar \
    docker.io/bitnami/grafana
EOF
chmod 755 /root/run-grafana

#############################
#
# Final tweaks
#
#############################

# disable automatic updates
systemctl disable --now zincati.service

rm -rf /var/log/journal/*

# reduce image size
/var/lib/testvm/zero-disk.setup
