#!/usr/bin/env python3

# This file is part of Cockpit.
#
# Copyright (C) 2020 Red Hat, Inc.
#
# Cockpit is free software; you can redistribute it and/or modify it
# under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation; either version 2.1 of the License, or
# (at your option) any later version.
#
# Cockpit is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Cockpit; If not, see <http://www.gnu.org/licenses/>.

import ssl
import sys
import sqlite3
import urllib.request
import argparse
import logging
import re

from datetime import datetime

from task import github, CA_PEM

# regexp for retry reason in a "not ok" line
re_retry = re.compile(r"# RETRY .* \(([^)]+)\)")

DATE_FORMAT = "%Y-%m-%dT%H:%M:%S%z"


def main():
    parser = argparse.ArgumentParser(description='Store information about failed tests')
    parser.add_argument("--db", default="cockpit-failures.db", help="Database name")
    parser.add_argument("--repo", help="Repository from which to process failures")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    parser.add_argument("revision", help="SHA for which failures should be stored")
    opts = parser.parse_args()

    if opts.verbose:
        logging.basicConfig(level=logging.DEBUG)

    ctx = ssl.create_default_context()
    ctx.load_verify_locations(cafile=CA_PEM)

    db_conn = sqlite3.connect(opts.db)
    cursor = db_conn.cursor()
    init_db(cursor)

    api = github.GitHub(repo=opts.repo)
    statuses = api.all_statuses(opts.revision)
    if not statuses:
        logging.warning("Revision %s has no statuses", opts.revision)
        return 0

    contexts = {}

    # Read the "status story"
    # First we get 'Not yet tested' or 'Not yet tested (direct trigger)'
    # Then when it is picked up we get "Testing in progress" and finally it is either red or green
    # Putting it all together can give us data like number of retries, how long it waited to get
    # testes, how long testing took...
    # Create map from 'context' to array of 'testrun's where each is a map containing: (order in the
    # array indicates retries, the first is original run, the second is the first retry...)
    # 'repo', 'url', 'state', 'description'
    # 'started' - when this run was picked by bot
    # 'posted' - when this run was triggered
    # 'finished' - when this run finished - 'state' is then either 'success', 'failure' or 'error'
    for status in reversed(statuses):
        repo = api.repo
        context = status["context"]
        if "@" in context:
            parts = context.split("@")
            context = parts[0]
            if not parts[1].startswith("bots"):
                # Can contain branch, as it diffs between master and branches (e.g `cockpit-project/cockpit/rhel-7.9`)
                repo = parts[1]

        cc = contexts.setdefault(context, [])

        desc = status["description"]
        if status["state"] == "pending":
            if "Testing in progress" in desc:
                cc[-1]["url"] = status["target_url"]
                cc[-1]["started"] = datetime.strptime(status["created_at"], DATE_FORMAT).timestamp()
            elif "Not yet tested" in desc:
                new_run = {"posted": datetime.strptime(status["created_at"], DATE_FORMAT).timestamp(), "repo": repo}
                cc.append(new_run)
            else:
                logging.warning("Pending status has unexpected description '{0}', skipping.".format(desc))
        elif status["state"] in ["success", "failure", "error"]:
            cc[-1]["finished"] = datetime.strptime(status["created_at"], DATE_FORMAT).timestamp()
            cc[-1]["state"] = status["state"]
            if status["state"] == "error":
                cc[-1]["description"] = desc
        else:
            logging.warning("Status has unexpected state '{0}', skipping.".format(status["state"]))

    for context, statuses in contexts.items():
        for retry, status in enumerate(statuses):
            if not status.get("finished"):
                # TODO: Store partial, store as non-finished?
                logging.debug("Test hasn't finished, skipping")
                continue

            url = status["url"]
            waited = status["started"] - status["posted"]
            took = status["finished"] - status["started"]
            logging.debug("Processing {0}{1} as {2} that waited {3}s and took {4}s in {5}. {6}".format(
                context,
                "(retry no." + str(retry) + ")" if retry else "",
                status["state"] + (" (" + status["description"] + ")" if status.get("description") else ""),
                waited, took, status["repo"], url
            ))

            row = cursor.execute("SELECT id FROM TestRuns WHERE url = ?", (url, )).fetchone()
            if row is not None:
                logging.warning("Test run %s already exists as ID %i, skipping", url, row[0])
                continue

            run_id = insert_run(cursor, status["repo"], opts.revision, context, url,
                                status["posted"], retry, waited, took, status["state"],
                                status.get("description"))

            raw_log = url[:-5] if url.endswith(".html") else url
            logging.debug("scanning log %s", raw_log)

            with urllib.request.urlopen(raw_log, context=ctx if "logs.cockpit-project.org" in raw_log else None) as fp:
                for (testname, retry_reason) in find_failed_tests(fp):
                    insert_failure(cursor, run_id, testname, retry_reason)

    db_conn.commit()
    db_conn.close()


def init_db(cursor):
    cursor.execute("""CREATE TABLE if not exists TestRuns
                    (id INTEGER PRIMARY KEY,
                     project TEXT,
                     revision TEXT,
                     context TEXT,
                     url TEXT,
                     time TIMESTAMP,
                     retry INTEGER,
                     wait_seconds INTEGER,
                     run_seconds INTEGER,
                     state TEXT,
                     description TEXT,
                     UNIQUE (url))
                   """)
    cursor.execute("""CREATE TABLE if not exists Failures
                    (testname TEXT NOT NULL,
                     retry_reason TEXT,
                     run INTEGER NOT NULL,
                     FOREIGN KEY (run) REFERENCES TestRuns(id))
                   """)


def insert_run(cursor, project, revision, context, url, time, retry, waited, took, state, desc):
    return cursor.execute("INSERT INTO TestRuns VALUES (null, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
                          (project, revision, context, url, time, retry, waited, took, state, desc)).lastrowid


def insert_failure(cursor, run_id, testname, retry_reason):
    cursor.execute("INSERT INTO Failures VALUES (?, ?, ?)", (testname, retry_reason, run_id))


def find_failed_tests(fp):
    last_msg = ""
    save_message = False
    for line in fp:
        line = line.decode('utf-8')

        if save_message:
            last_msg = line.strip()
            save_message = False
            continue

        # If the issue is known, don't save unexpected messages
        if line.startswith("ok ") and "SKIP Known issue" in line:
            save_message = False
            last_msg = ""

        if "FAIL: Test completed, but found unexpected" in line and "raise" not in line:
            save_message = True

        if line.startswith("not ok "):
            # Result line has form like:
            # not ok 24 test/verify/check-foobar TestExample.testBasic [ND] # RETRY 2 (be robust against unstable tests)
            # older logs look like this instead:
            # not ok 78 testNodeNavigation (check_openshift.TestOpenshift) # duration: 172s
            fields = line.split(" ")
            # sometimes line breaks are missing:
            # not ok 117 test/verify/check-metrics TestMetrics.testPcp# ------
            fields[4] = fields[4].rstrip("#")
            testname = "{0} {1}".format(fields[3], fields[4])
            m = re_retry.search(line)
            yield (testname, last_msg or (m and m.group(1) or None))
            last_msg = ""


if __name__ == '__main__':
    sys.exit(main())
